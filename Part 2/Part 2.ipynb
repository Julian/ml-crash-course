{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Crash Course: Model Families and Hyperparameters\n",
        "\n",
        "In our second notebook, we're going to begin to explore some of the internals that were handled for us when using H2O's AutoML. We won't fully work through prediction like we did in the first portion, since our goal is to drop down just one layer of abstraction in order to understand some of what occured automatically for us, not to fully immerse ourselves in this level of detail.\n",
        "\n",
        "### Start H2O\n",
        "\n",
        "Import the **h2o** Python module and `H2OAutoML` class and initialize a local H2O cluster. If you ran part 1's notebook, this should detect the still-running H2O cluster that was started earlier."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import h2o\n",
        "from h2o.automl import H2OAutoML\n",
        "h2o.init()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data\n",
        "\n",
        "Just as we did when letting H2O figure out what models to build for us, we're going to load a dataset. This time, we'll use a subset of a publically available [Single-family Loan dataset from Freddie Mac](http://www.freddiemac.com/fmac-resources/research/pdf/user_guide.pdf).  Here we'll predict whether or not a loan holder will default on their loan."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = h2o.import_file(\"https://s3.amazonaws.com/data.h2o.ai/DAI-Tutorials/loan_level_500k.csv\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's again inspect the dataset we've loaded to see what columns it contains, and compare them with the user guide linked above."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also again investigate the distributions and types of the columns we have."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The target we'll predict will be the `DELINQUENT` column, which indicates whether the loan holder in a given row was delinquent on their loan. Let's look at the distribution of its values."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"DELINQUENT\"].table()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see we have many more non-delinquent loan holders than delinquent ones. It's often important to notice highly skewed or imbalanced target variables, and there are techniques for \"fixing\", \"modifying\", or \"rebalancing\" this kind of imbalance in order to improve the quality of a trained model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first \"new\" thing we'll do, which previously H2O had done for us, is to split our dataset into a few parts."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "training, validation, test = df.split_frame([0.7, 0.15])\n",
        "print(\n",
        "    \"\"\"\n",
        "    training rows: %d\n",
        "    validation rows: %d\n",
        "    test rows: %d\n",
        "    \"\"\" % (training.nrows, validation.nrows, test.nrows)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to previously, we specify the columns that will be used as features, and our target column."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "y = \"DELINQUENT\"\n",
        "\n",
        "ignore = [\"DELINQUENT\", \"PREPAID\", \"PREPAYMENT_PENALTY_MORTGAGE_FLAG\", \"PRODUCT_TYPE\"] \n",
        "\n",
        "x = list(set(training.names) - set(ignore))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Specific Model Family: Boosted Trees\n",
        "\n",
        "We're going to build a model of a specific type, a **gradient boosted tree**."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import h2o.estimators.gbm\n",
        "gbm = h2o.estimators.gbm.H2OGradientBoostingEstimator()\n",
        "gbm.train(x=x, y=y, training_frame=training, validation_frame=validation)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can investigate the specific GBM model that we created:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "gbm"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And as before, we can run predictions, here on our validation set:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "gbm.predict(validation)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Many model families have what are called \"hyperparameters\".\n",
        "\n",
        "Hyperparameters are loosely parameters to the model building itself, as opposed to parameters that are learned during training the model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "hyper_params = {'max_depth' : [1,3,5,6,7,8,9,10,12,13,15]}\n",
        "\n",
        "gbm = h2o.estimators.gbm.H2OGradientBoostingEstimator(ntrees=150)\n",
        "gbm_grid = h2o.grid.H2OGridSearch(\n",
        "    gbm,\n",
        "    hyper_params,\n",
        "    search_criteria={\"strategy\":\"Cartesian\"},\n",
        ")\n",
        "\n",
        "gbm_grid.train(\n",
        "    x=x, y=y, training_frame=training, validation_frame=validation,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_gbm_depth = gbm_grid.get_grid(sort_by='auc',decreasing=True)\n",
        "sorted_gbm_depth"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise\n",
        "\n",
        "Use the AutoML techniques we learned in part 1 to build an AutoML model for this new dataset. How does it compare to the model we built manually?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from h2o.automl import H2OAutoML"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The contents of this notebook were lightly adapted from a tutorial in the [official H2O AutoML documentation](https://h2oai.github.io/tutorials/introduction-to-machine-learning-with-h2o-part-1/). There are many more interesting tutorials to work through there. Explore them!"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.15.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}